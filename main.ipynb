{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>word katandandre food crapilicious mkr</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>xochitlsuckkks classy whore red velvet cupcake</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>meh thanks head concerned another angry dude t...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>rudhoeenglish isi account pretending kurdish a...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  \\\n",
       "0           0             word katandandre food crapilicious mkr   \n",
       "1           1  aussietv white mkr theblock imacelebrityau tod...   \n",
       "2           2     xochitlsuckkks classy whore red velvet cupcake   \n",
       "3           3  meh thanks head concerned another angry dude t...   \n",
       "4           4  rudhoeenglish isi account pretending kurdish a...   \n",
       "\n",
       "  cyberbullying_type  \n",
       "0  not_cyberbullying  \n",
       "1  not_cyberbullying  \n",
       "2  not_cyberbullying  \n",
       "3  not_cyberbullying  \n",
       "4  not_cyberbullying  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('cyber_bully_cleaned.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44005\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for i, tweet in enumerate(data[\"tweet_text\"]):\n",
    "    for word in tweet.split():\n",
    "        unique_words.add(word)\n",
    "\n",
    "print(len(unique_words))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22276\n"
     ]
    }
   ],
   "source": [
    "for word in list(unique_words):\n",
    "    if word not in word2vec_model.key_to_index:\n",
    "        unique_words.remove(word)\n",
    "        \n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22277, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix =[[0] * 300] #this matrix will contain the word embeddings for the top 5000 words #the first row will be the embedding for the padding token which is 0\n",
    "word_indices = {} #this dictionary will map each word to its index in the embedding matrix, this is needed when we train the neural network, the input to the neural network will be the index of the word in the embedding matrix\n",
    "idx = 1\n",
    "for i, word in enumerate(unique_words):\n",
    "    if word in word2vec_model.key_to_index:\n",
    "        embedding_matrix.append(word2vec_model[word])\n",
    "        word_indices[word] = idx\n",
    "        idx += 1\n",
    "    \n",
    "    else:\n",
    "        continue\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM \n",
    "class LSTMClf(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix, device):\n",
    "        super(LSTMClf, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # self.embedding = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])\n",
    "        # self.embedding.load_state_dict({'weight': torch.FloatTensor(embedding_matrix)})\n",
    "        # self.embedding.weight.requires_grad = False #freeze the embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #randomly initialize the hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :]).relu()\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class RNNClf(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix, device):\n",
    "        super(RNNClf, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])\n",
    "        self.embedding.load_state_dict({'weight': torch.FloatTensor(embedding_matrix)})\n",
    "        self.embedding.weight.requires_grad = False #freeze the embedding layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) \n",
    "        self.fc = nn.Linear(hidden_size, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.rnn(out, h0)\n",
    "        out = self.fc(out[:, -1, :]).relu()\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "class CNNClf(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_matrix):\n",
    "        super(CNNClf, self).__init__()\n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])\n",
    "        self.embedding.load_state_dict({'weight': torch.FloatTensor(embedding_matrix)})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size=3) #output size = (input_size - kernel_size + 1) = (300 - 3 + 1) = 298 \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 298, 64)  # 298 is the output size of the convolutional layer\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.conv1(out).relu()\n",
    "        out = out.flatten(1)\n",
    "        out = self.fc(out).relu()\n",
    "        out = self.fc2(out).relu()\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class BiLSTMClf(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, embedding_matrix, device):\n",
    "        super(BiLSTMClf, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])\n",
    "        self.embedding.load_state_dict({'weight': torch.FloatTensor(embedding_matrix)})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True) \n",
    "        self.fc = nn.Linear(hidden_size * 2, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0  = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(self.device)\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out, (h0, c0)) \n",
    "        out = self.fc(out[:, -1, :]).relu()\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sequence, max_len):\n",
    "    if len(sequence) <= max_len:\n",
    "        sequences = [sequence + [0] * (max_len - len(sequence))]\n",
    "    \n",
    "    elif len(sequence) > max_len:\n",
    "        #break sequence into chunks of max_len\n",
    "        sequences = []\n",
    "        while len(sequence) > max_len:\n",
    "            sequences.append(sequence[:max_len])\n",
    "            sequence = sequence[max_len:]\n",
    "        #if the last chunk is less than max_len, then pad it with zeros\n",
    "        if len(sequences[-1]) < max_len and len(sequences[-1]) > 5:\n",
    "            sequences[-1] += [0] * (max_len - len(sequences[-1]))\n",
    "    \n",
    "    for seq in sequences:\n",
    "        assert len(seq) == max_len\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, word_indices, max_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        sequence = [word_indices[word] for word in row[\"preprocessed_text\"] if word in word_indices and word in word2vec_model.key_to_index]\n",
    "        sequences = padding(sequence, max_len)\n",
    "        X += sequences\n",
    "        if row[\"subreddit\"] == \"climate_science\":\n",
    "            y += [0] * len(sequences)\n",
    "        else:\n",
    "            y += [1] * len(sequences)\n",
    "    \n",
    "    # print(len(X[2]))\n",
    "    #convert X and y to numpy arrays then to tensors\n",
    "    X = torch.tensor(X).clone().detach()\n",
    "    y = torch.tensor(y).float().clone().detach()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data  \n",
    "batch_size = 32\n",
    "max_len = 100\n",
    "X_train, y_train = prepare_data(data[:-406], word_indices, max_len)\n",
    "num_0_class = len(y) - sum(y)\n",
    "num_1_class = sum(y)\n",
    "print(num_0_class, num_1_class)\n",
    "\n",
    "X_test, y_test = prepare_data(data[-406:], word_indices, max_len)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "#find number of each class in the training set\n",
    "\n",
    "#convert the data into data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# val_dataset = TensorDataset(X_val, y_val)\n",
    "# val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(train_loader), len(test_loader))\n",
    "\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, validation_data, criterion, optimizer, device, model, num_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_corr = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for x, y in train_data:\n",
    "            # Send data to the correct device\n",
    "            x, y = x.to(device), y.to(device).float()\n",
    "            y = y.unsqueeze(1)  # Ensure y has correct shape for binary classification\n",
    "            total_train += y.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item() \n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_corr += ((output > 0.5).float() == y).sum().item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss / len(train_data))\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_data:\n",
    "                x, y = x.to(device), y.to(device).float()\n",
    "                y = y.unsqueeze(1)\n",
    "                output = model(x)\n",
    "                val_loss += criterion(output, y).item()\n",
    "            \n",
    "            val_losses.append(val_loss / len(validation_data))\n",
    "        \n",
    "        # Display metrics\n",
    "        train_accuracy = 100 * train_corr / total_train\n",
    "        # print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_losses[-1]:.4f}, \"\n",
    "        #       f\"Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def test(test_data, model, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_data:\n",
    "            x, y = x.to(device), y.to(device).float()\n",
    "            output = model(x).squeeze(1)\n",
    "            \n",
    "            # Collect predictions and true labels as lists of integers\n",
    "            y_pred.extend((output > 0.5).int().cpu().tolist())\n",
    "            y_true.extend(y.int().cpu().tolist())\n",
    "\n",
    "    # Ensure that y_true is the first argument in the metric functions\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "input_size = 300\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "num_classes = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_clf = LSTMClf(input_size, hidden_size, num_layers, num_classes, embedding_matrix, device).to(device)\n",
    "rnn_clf = RNNClf(input_size, hidden_size, num_layers, num_classes, embedding_matrix, device).to(device)\n",
    "cnn_clf = CNNClf(100, hidden_size, embedding_matrix).to(device)\n",
    "bi_lstm_clf = BiLSTMClf(input_size, hidden_size, num_layers, num_classes, embedding_matrix, device).to(device)\n",
    "\n",
    "#optimizer\n",
    "models = [lstm_clf, cnn_clf, rnn_clf, bi_lstm_clf]\n",
    "dl_results = {}\n",
    "for model in models:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "    train_losses, val_losses = train(train_loader, test_loader, criterion, optimizer, device, model, num_epochs)\n",
    "    accuracy, precision, recall, f1 = test(test_loader, model, device)\n",
    "    dl_results[model.__class__.__name__] = {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    print(\"\\n\\n\")\n",
    "    #plot the training and validation losses\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
